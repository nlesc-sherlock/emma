---
sbt_version: 0.13.15
scala_release: 2.11
scala_version: 2.11.8
spark_prev_version: "2.1.1"
spark_version: "2.1.1"
spark_mirror: "http://d3kbcqa49mib13.cloudfront.net"
spark_dir: "/data/local/spark"
spark_src_dir: "/usr/local/src"
spark_conf_dir: "/etc/spark"
spark_usr_parent_dir: "/usr/lib"  #this is the folder where the spark archive will be extracted
spark_usr_dir: "/usr/lib/spark"   #this is the symlink to the extracted/installed spark
spark_lib_dir: "/var/lib/spark"
spark_log_dir: "/var/log/spark"
spark_run_dir: "/run/spark"
spark_user: "spark"           # the name of the (OS)user created for spark
spark_user_groups: [ users ]         # Optional list of (OS)groups the new spark user should belong to
spark_user_shell: "/bin/false"    # the spark user's default shell
spark_memory_fraction: 0.6
spark_memory_storageFraction: 0.5
spark_memory_offheap_enable: true
spark_memory_offheap_size: 2g
spark_memory_uselegacymode: true
spark_executor_cores: 2
spark_executor_memory: 2g
spark_worker_cores: 2
spark_worker_memory: 2g
spark_driver_memory: 1g
spark_daemon_memory: 3g
spark_driver_maxResultSize: 1g
spark_dynamicAllocation_initialExecutors: 1
spark_dynamicAllocation_maxExecutors: 2
spark_broadcast_blockSize: 20m
spark_task_cpus: 2

joda_time_version: "2.9.4"
aws_version: "1.10.6"
httpclient_version: "4.5.3"
hadoop_aws_version: "2.8.0"
spark_classpath_extras: [ "joda-time-{{ joda_time_version }}.jar", "aws-java-sdk-s3-{{ aws_java_sdk_s3_version }}.jar", "hadoop-aws-{{ hadoop_aws_version }}.jar" ]
spark_env_extras: {}
spark_defaults_extras: { spark.kryoserializer.buffer 256m, spark.kryoserializer.buffer.max 512m, spark.shuffle.service.enabled true, spark.shuffle.service.port 7338 }

worker_debug_port: 5006
driver_debug_port: 5005
worker_waiting_on_startup: n
driver_waiting_on_startup: n
gc_debug: "" #"-XX:+PrintFlagsFinal -XX:+PrintReferenceGC -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+PrintAdaptiveSizePolicy"
gc_type: "+UseG1GC" #Or +UseParallelGC
spark_debug_mode: false

