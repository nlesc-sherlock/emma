# Default system properties included when running spark-submit.
# This is useful for setting default environmental settings.

# Example:
# spark.master                     spark://master:7077
# spark.eventLog.enabled           true
# spark.eventLog.dir               hdfs://namenode:8021/directory
# spark.serializer                 org.apache.spark.serializer.KryoSerializer
# spark.driver.memory              5g
{% if spark_debug_mode == true %}
spark.executor.extraJavaOptions -XX:{{ gc_type }} -XX:+UseCompressedOops {{ gc_debug }}  -agentlib:jdwp=transport=dt_socket,server=y,suspend={{ worker_waiting_on_startup }},address={{ worker_debug_port }}
spark.driver.extraJavaOptions -XX:{{ gc_type }} -XX:+UseCompressedOops {{ gc_debug }} -agentlib:jdwp=transport=dt_socket,server=y,suspend={{ driver_waiting_on_startup }},address={{ driver_debug_port }}
{% else %}
spark.executor.extraJavaOptions -XX:{{ gc_type }} -XX:+UseCompressedOops 
spark.driver.extraJavaOptions -XX:{{ gc_type }} -XX:+UseCompressedOops
{% endif %}

spark.master    spark://{{ groups['spark-master'][0] }}:7077
spark.shuffle.service.enabled true
spark.dynamicAllocation.enabled   true
spark.dynamicAllocation.initialExecutors  {{ spark_dynamicAllocation_initialExecutors }}
spark.dynamicAllocation.maxExecutors {{ spark_dynamicAllocation_maxExecutors }}
spark.executor.cores={{ spark_executor_cores }}
spark.eventLog.enabled true
spark.eventLog.dir {{ spark_dir }}/spark-events
spark.broadcast.blockSize {{ spark_broadcast_blockSize }}
spark.memory.fraction {{ spark_memory_fraction }}
spark.memory.storageFraction {{ spark_memory_storageFraction }}
spark.memory.offHeap.enable {{ spark_memory_offheap_enable }}
spark.memory.offHeap.size {{ spark_memory_offheap_size }}
spark.memory.useLegacyMode {{ spark_memory_uselegacymode }}
spark.executor.memory {{ spark_executor_memory }}
spark.worker.memory {{ spark_worker_memory }}
spark.driver.memory {{ spark_driver_memory }}
spark.daemon.memory {{ spark_daemon_memory }}
spark.driver.maxResultSize {{ spark_driver_maxResultSize }}
#spark.submit.deployMode cluster
spark.serializer org.apache.spark.serializer.KryoSerializer
spark.scheduler.mode FAIR
spark.driver.allowMultipleContexts true
spark.task.cpus {{ spark_task_cpus }}

#Configuration for S3
spark.hadoop.fs.s3a.impl org.apache.hadoop.fs.s3a.S3AFileSystem
spark.hadoop.fs.s3a.endpoint http://{{ lookup('pipe', 'getent ' 'hosts {{ inventory_hostname }} | awk \'{ print $1 }\'')}}{{ minio_server_addr }}
spark.hadoop.fs.s3a.access.key {{ minio_access_key }}
spark.hadoop.fs.s3a.secret.key {{ minio_secret_key }}
spark.hadoop.fs.s3a.connection.ssl.enabled false
spark.hadoop.fs.s3a.fs.s3a.fast.upload true
spark.hadoop.fs.s3a.buffer.dir /root/spark/work,/tmp
spark.hadoop.fs.s3a.path.style.access true

#Jars
spark.jars
spark.jars.packages
spark.jars.ivy


#ExtraClass
{% for jar in spark_classpath_extras | sort %}
{%- if loop.first %}spark.executor.extraClassPath {% endif -%}{{ spark_usr_dir  }}/jars/{{ jar }}{%- if not loop.last %}:{% endif -%}
{%- endfor %}


{% for jar in spark_classpath_extras | sort %}
{%- if loop.first %}spark.driver.extraClassPath {% endif -%}{{ spark_usr_dir  }}/jars/{{ jar }}{%- if not loop.last %}:{% endif -%}
{%- endfor %}


#Extra configurations from the defaults in the spark role
{% for key, value in spark_defaults_extras.items() | sort %}
{{ key }} {{ value }}
{% endfor %}
