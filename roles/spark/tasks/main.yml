---
#Create environment for Spark
- name: R ppa key
  apt_key: id="{{ cran_ppa_key }}" state=present keyserver=keyserver.ubuntu.com
- name: R ppa
  apt_repository: repo='deb {{ cran_url }}/bin/linux/ubuntu xenial/' state=present
- name: Update apt cache
  apt: update_cache=yes
- name: numpy
  apt: name=python-numpy
- name: libnet
  apt: name=libnetlib-java
- name: R
  apt: name=r-recommended
- name: R javareconf
  command: R CMD javareconf
- name: RJava
  apt: name=r-cran-rjava
- name: Gradle
  apt: name=gradle

- name: Get Scala
  get_url: url="http://www.scala-lang.org/files/archive/scala-{{ scala_version }}.deb"
           dest="/tmp/scala-{{ scala_version }}.deb"

- name: Install Scala
  shell: "dpkg -r scala ; dpkg -i /tmp/scala-{{ scala_version }}.deb"

#Setting up Spark
- name: Create service account for Spark
  user: name={{ spark_user }}
        system=yes
        home={{ spark_lib_dir }}
        shell={{ spark_user_shell }}
        state=present
        groups="{{ spark_user_groups | join(',') }}"
  tags: ["spark-user"]

- stat: path="{{ spark_src_dir }}/spark-{{ spark_version }}-bin-without-hadoop.tgz"
  register: spark_dirs

- name: Remove Spark directories if we are installing a new version
  file:
    state: absent
    path: "{{ item }}"
  with_items:
    - "{{ spark_dir }}"
    - "{{ spark_log_dir }}"
    - "{{ spark_run_dir }}"
    - "{{ spark_work_dir }}"
    - "{{ spark_usr_parent_dir }}/spark-{{ spark_version }}-bin-without-hadoop/"
    - "{{ spark_usr_parent_dir }}/spark-{{ spark_prev_version }}-bin-without-hadoop/"
  when: spark_dirs.stat.exists is defined and not spark_dirs.stat.exists

- name: Find Spark tars to be removed if we are installing a new version.
  find:
    paths: "{{ spark_src_dir }}/"
    patterns: "spark*"
  register: spark_tars_to_delete
  when: spark_dirs.stat.exists is defined and not spark_dirs.stat.exists

- name: Remove Spark tars if we are installing a new version
  file:
    path: "{{ item.path }}"
    state: absent
  with_items: "{{ spark_tars_to_delete.files }}"
  when: spark_dirs.stat.exists is defined and not spark_dirs.stat.exists

- name: Remove symlinks
  file:
    path: "{{ item }}"
    state: absent
  with_items:
    - "{{ spark_usr_dir }}"
    - "{{ spark_conf_dir }}/conf"
  when: spark_dirs.stat.exists is defined and not spark_dirs.stat.exists

- name: Remove the shortcuts
  file:
    state: absent
    path: "{{ item }}"
  with_items:
    - /usr/bin/spark-class
    - /usr/bin/spark-shell
    - /usr/bin/spark-sql
    - /usr/bin/spark-submit
    - /usr/bin/pyspark
  when: spark_dirs.stat.exists is defined and not spark_dirs.stat.exists

- stat: path="{{ spark_src_dir }}/spark-{{ spark_version }}-bin-without-hadoop.tgz"
  register: spark_dirs_sci
  tags: ["scispark"]

- name: Remove SciSpark jar so it is re-compiled for the new Spark version
  file:
    state: absent
    path: "{{ item }}"
  with_items:
    - "{{ scispark_dir }}/SciSpark/target/scala-{{ scala_release }}/SciSpark.jar"
    - "{{ spark_usr_dir }}/jars/SciSpark.jar"
  when: spark_dirs_sci.stat.exists is defined and not spark_dirs_sci.stat.exists
  tags: ["scispark"]

- name: Ensure Spark directory exists
  file: path="{{ spark_dir }}" state=directory owner=ubuntu group={{ spark_user_groups[0] }} mode=g+rw

- name: Ensure Spark events directory exists
  file: path={{ spark_dir }}/spark-events state=directory owner={{ spark_user }} group={{ spark_user_groups[0] }} mode=g+rw

- name: Ensure Spark configuration directory exists
  file: path="{{ spark_conf_dir }}"
        state=directory
  tags: ["config"]

- name: Ensure Spark log, run, and work directories exist
  file: path="{{ item }}"
        owner={{ spark_user }}
        group={{ spark_user_groups[0] }}
        mode=0755
        state=directory
  with_items:
    - "{{ spark_log_dir }}"
    - "{{ spark_run_dir }}"
    - "{{ spark_work_dir }}"

- name: Download Spark distribution
  get_url: url="{{ spark_mirror }}/spark-{{ spark_version }}-bin-without-hadoop.tgz"
           dest="{{ spark_src_dir }}/spark-{{ spark_version }}-bin-without-hadoop.tgz"

- name: Extract Spark distribution
  unarchive: src="{{ spark_src_dir }}/spark-{{ spark_version }}-bin-without-hadoop.tgz"
             dest="{{ spark_usr_parent_dir }}"
             copy=no
             creates="{{ spark_usr_parent_dir }}/spark-{{ spark_version }}-bin-without-hadoop"

- name: Setup Spark distribution symlinks
  file: src="{{ item.src }}"
        dest="{{ item.dest }}"
        state=link
  with_items:
    - { src: "{{ spark_usr_parent_dir }}/spark-{{ spark_version }}-bin-without-hadoop", dest: "{{ spark_usr_dir }}" }
    - { src: "{{ spark_usr_parent_dir }}/spark-{{ spark_version }}-bin-without-hadoop/conf", dest: "{{ spark_conf_dir }}/conf" }
  tags: ["symlinks"]

- name: Create shims for Spark binaries
  template: src=spark-shim.j2
            dest="/usr/bin/{{ item }}"
            mode=0755
  with_items:
    - spark-class
    - spark-shell
    - spark-sql
    - spark-submit
    - pyspark
  tags: ["shims"]

#Downland necessary jars using maven
- name: Maven joda-time
  maven_artifact:
    group_id: joda-time
    artifact_id: joda-time
    version: "{{ joda_time_version }}"
    repository_url: http://central.maven.org/maven2/
    dest: "{{ spark_usr_dir }}/jars/joda-time-{{ joda_time_version }}.jar"

- name: Maven hadoop-aws
  maven_artifact:
    group_id: org.apache.hadoop
    artifact_id: hadoop-aws
    version: "{{ hadoop_aws_version }}"
    repository_url: http://central.maven.org/maven2/
    dest: "{{ spark_usr_dir }}/jars/hadoop-aws-{{ hadoop_aws_version }}.jar"

- name: Maven aws-java-sdk-s3
  maven_artifact:
    group_id: com.amazonaws
    artifact_id: aws-java-sdk-s3
    version: "{{ aws_version }}"
    repository_url: http://central.maven.org/maven2/
    dest: "{{ spark_usr_dir }}/jars/aws-java-sdk-s3-{{ aws_version }}.jar"

- name: Maven aws-java-sdk-core
  maven_artifact:
    group_id: com.amazonaws
    artifact_id: aws-java-sdk-core
    version: "{{ aws_version }}"
    repository_url: http://central.maven.org/maven2/
    dest: "{{ spark_usr_dir }}/jars/aws-java-sdk-core-{{ aws_version }}.jar"

- name: Maven aws-java-sdk-kms
  maven_artifact:
    group_id: com.amazonaws
    artifact_id: aws-java-sdk-kms
    version: "{{ aws_version }}"
    repository_url: http://central.maven.org/maven2/
    dest: "{{ spark_usr_dir }}/jars/aws-java-sdk-kms-{{ aws_version }}.jar"

- name: Maven aws-java-sdk
  maven_artifact:
    group_id: com.amazonaws
    artifact_id: aws-java-sdk
    version: "{{ aws_version }}"
    repository_url: http://central.maven.org/maven2/
    dest: "{{ spark_usr_dir }}/jars/aws-java-sdk-{{ aws_version }}.jar"

- name: Maven httpclient
  maven_artifact:
    group_id: org.apache.httpcomponents
    artifact_id: httpclient
    version: "{{ httpclient_version }}"
    repository_url: http://central.maven.org/maven2/
    dest: "{{ spark_usr_dir }}/jars/httpclient-{{ httpclient_version }}.jar"

- name: Maven jmespath-java
  maven_artifact:
    group_id: com.amazonaws
    artifact_id: jmespath-java
    version: 1.0
    repository_url: http://central.maven.org/maven2/
    dest: "{{ spark_usr_dir }}/jars/jmespath-java-1.0.jar"
  when: ( aws_version == "3.0.0-apha1") or (aws_version == "3.0.0-apha2")

- name: Maven wisp
  maven_artifact:
    group_id: com.quantifind
    artifact_id: wisp_2.11
    version: 0.0.4
    repository_url: http://central.maven.org/maven2/
    dest: "{{ spark_usr_dir }}/jars/wisp_2.11-0.0.4.jar"
  when: ( spark_version == "2.2.0")

- name: Configure Spark environment
  template: src=spark-env.sh.j2
            dest="{{ spark_usr_parent_dir }}/spark-{{ spark_version }}-bin-without-hadoop/conf/spark-env.sh"
  tags: ["config"]

- name: Configure Spark defaults config file
  template: src=spark-defaults.conf.j2
            dest="{{ spark_usr_parent_dir }}/spark-{{ spark_version }}-bin-without-hadoop/conf/spark-defaults.conf"
  tags: ["config"]

- name: Configure Hive defaults site file
  template: src=hive-site.xml.j2
            dest="{{ spark_usr_parent_dir }}/spark-{{ spark_version }}-bin-without-hadoop/conf/hive-site.xml"
  tags: ["config"]

#Install Numpy & SciPy and its dependencies
- name: Install pip packages
  pip: name={{ item }} executable=pip3
  with_items:
    - numpy
    - scipy
    - matplotlib
    - ipython
    - pandas
    - sympy
    - nose

- name: Install Python3-tk to have module _tkinter
  apt: name=python3-tk

#Install and enable ATLAS and OpenBLAS for netlib-java to be used by Breeze
#Breeze is a library for linear algebra operations used by SparkML
- name: Install Atlas and OpenBLAS packages
  apt: name={{ item }}
  with_items:
    - libatlas3-base
    - libopenblas-base
    - libblas-dev
    - liblapack-dev
- name: Configure Atlas and OpenBLAS
  shell: "{{ item }}"
  with_items:
    - sudo update-alternatives --config libblas.so
    - sudo update-alternatives --config libblas.so.3
    - sudo update-alternatives --config liblapack.so
    - sudo update-alternatives --config liblapack.so.3

#Spark Master
- name: Spark config slaves
  template:
    src: slaves.j2
    dest: '{{ spark_conf_dir }}/conf/slaves'
  when: inventory_hostname in groups['spark-master']

- name: Copy start-master.systemd
  template: src=spark-master.systemd.j2 dest=/etc/systemd/system/spark-master.service
  when: inventory_hostname in groups['spark-master']

- name: Copy start-slave.systemd
  template: src=spark-slave.systemd.j2 dest=/etc/systemd/system/spark-slave.service

- name: Copy start-shuffle.systemd
  template: src=spark-shuffle.systemd.j2 dest=/etc/systemd/system/spark-shuffle.service

- name: Copy build.gradle
  copy: src=build.gradle dest=/home/ubuntu/build.gradle

- name: Gradle install dependencies
  shell: gradle downloadDependencies
